{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, dropout):\n",
    "    '''\n",
    "    query, key, value: (b, h, n, d)\n",
    "    p_attn: (b, h, n, n)\n",
    "    result: (b, h, n, d)\n",
    "    '''\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    p_attn = dropout(p_attn)\n",
    "    result = torch.matmul(p_attn, value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self, h, d_model, p_dropout):\n",
    "        '''\n",
    "        h: the number of head\n",
    "        d_model: the dimensions of input token vector\n",
    "        p_dropout: probability of dropout \n",
    "        '''\n",
    "        \n",
    "        super(MSA, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        \n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.token_linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.fin_linear = nn.Linear(d_model, d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        num_batches = z.size(0)\n",
    "        \n",
    "        query, key, value = \\\n",
    "            [l(z).view(num_batches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l in self.token_linears ]\n",
    "        \n",
    "        attn = attention(query, key, value, self.dropout)\n",
    "        \n",
    "        attn = attn.transpose(1, 2).contiguous().view(num_batches, -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.fin_linear(attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model , d_ff, p_dropout):\n",
    "        '''\n",
    "        d_model: the dimensions of input token vector\n",
    "        d_ff: intermediate dimensions of linear mapping\n",
    "        p_dropout: probability of dropout \n",
    "        '''\n",
    "        \n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.w_1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.w_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layer, num_head, dim_model, p_dropout, d_ff):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.msa_list = nn.ModuleList([MSA(num_head, dim_model, p_dropout) for _ in range(num_layer)])\n",
    "        self.ffn_list = nn.ModuleList([FFN(dim_model, d_ff, p_dropout) for _ in range(num_layer)])\n",
    "        \n",
    "    def forward(self, z):\n",
    "        for msa, ffn in zip(self.msa_list, self.ffn_list):\n",
    "            z = msa(z) + z\n",
    "            z = ffn(z) + z\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=16, num_cls=10, dim_model=128, num_layer=12, num_head=8, p_dropout=0.1, d_ff=256):\n",
    "        super(ViT, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.num_patch = (img_size // patch_size) ** 2 \n",
    "        assert img_size % patch_size == 0 , 'img size must be divisible by patch size'    \n",
    "        self.patch_dim = patch_size * patch_size * 3\n",
    "        self.patch_size = patch_size\n",
    "        self.pos = nn.Parameter(torch.randn(1, self.num_patch + 1, dim_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim_model))\n",
    "        self.emb = nn.Linear(self.patch_dim, dim_model)\n",
    "        self.transformer = Transformer(num_layer, num_head, dim_model, p_dropout, d_ff)\n",
    "        self.mlp_head = nn.Linear(dim_model, num_cls)\n",
    "            \n",
    "\n",
    "    def forward(self, x, is_fine=False):\n",
    "        # img to patch\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(x, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = self.patch_size, p2 = self.patch_size)\n",
    "        x = self.emb(x)\n",
    "        cls_token = repeat(self.cls_token, '1 n d -> b n d', b = x.shape[0])\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        if not is_fine:\n",
    "            x += self.pos\n",
    "        else:\n",
    "            assert h % self.patch_size == 0 and w % self.patch_size == 0 , 'img size must be divisible by patch size'\n",
    "            inp_h = h // self.patch_size\n",
    "            inp_w = w // self.patch_size\n",
    "            \n",
    "            pos = rearrange(self.pos[:, 1:, :], 'b (h w) d -> b d h w', w=self.img_size // self.patch_size)\n",
    "            pos = F.interpolate(pos, (inp_w, inp_h), mode='bilinear')\n",
    "            pos = rearrange(pos, 'b d h w -> b (h w) d')\n",
    "            pos = torch.cat([self.pos[:, :1, :], pos], dim=1)\n",
    "            x += pos\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        y = self.mlp_head(x[:, 0, :])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "vit = ViT()\n",
    "x = torch.randn((1, 3, 128, 128))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "y = vit(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "high_resolution_x = torch.randn((1, 3, 256, 256))\n",
    "print(high_resolution_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jongyeon/envs/torch_vir/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    }
   ],
   "source": [
    "y = vit(x, is_fine=True)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "012d1490d375476291df6a84c69f0afa3c78de8b520d65d9d0b6ca5a3f1dea17"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torch_vir': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
